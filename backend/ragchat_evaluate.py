import sys
import json
import os
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
    retry_if_exception_type
)
import httpx
from datasets import Dataset, load_from_disk
from ragas import evaluate
from ragas.metrics import ContextRelevance, Faithfulness, AnswerRelevancy
from ragas.llms.base import BaseRagasLLM
from llama_index.llms.openai_like import OpenAILike

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("NvidiaDeepSeek")

# é¡¹ç›®æ ¹ç›®å½•è®¾ç½®
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
try:
    from backend import utils
except ImportError:
    logger.warning("æœªæ‰¾åˆ°backend.utilsæ¨¡å—")

# ================== é…ç½®åŠ è½½ ==================
CONFIG_PATH = "config.json"
with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
    MODEL_CONFIG = json.load(f)

# ================== æ™ºèƒ½å¹¶å‘æ§åˆ¶å™¨ ==================
class AdaptiveConcurrencyController:
    def __init__(self):
        self.semaphore = asyncio.Semaphore(1)  # åˆå§‹å¹¶å‘æ•°
        self.current_delay = 0.0
        self.max_permits = 2  # æœ€å¤§å¹¶å‘æ•°

    async def adjust(self, success: bool):
        """åŠ¨æ€è°ƒæ•´å¹¶å‘ç­–ç•¥"""
        if success:
            self.current_delay = max(0.0, self.current_delay - 0.5)
            if self.semaphore._value < self.max_permits:
                new_permits = self.semaphore._value + 1
                self.semaphore = asyncio.Semaphore(new_permits)
                logger.info(f"âœ… å¹¶å‘æ•°æå‡è‡³ {new_permits}")
        else:
            self.current_delay += 1.0
            new_permits = max(1, self.semaphore._value - 1)
            self.semaphore = asyncio.Semaphore(new_permits)
            logger.warning(f"âš ï¸ å¹¶å‘æ•°é™è‡³ {new_permits} å½“å‰å»¶è¿Ÿ {self.current_delay:.1f}s")

    async def __aenter__(self):
        await asyncio.sleep(self.current_delay)
        await self.semaphore.acquire()
        return self

    async def __aexit__(self, *args):
        self.semaphore.release()

# ================== NVIDIA DeepSeeké€‚é…å™¨ ==================
class NvidiaDeepSeekRagasLLM(BaseRagasLLM):
    def __init__(self, deepseek_llm: OpenAILike):
        super().__init__()
        self.llm = deepseek_llm
        self.concurrency_ctl = AdaptiveConcurrencyController()
        self._validate_endpoint()
        
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.llm.api_key}",
            "x-api-source": "python-sdk",
            "accept": "application/json"
        }
        logger.info(f"ğŸ”§ åˆå§‹åŒ–æˆåŠŸ | æ¨¡å‹: {self.llm.model}")

    def _validate_endpoint(self):
        """ä¸¥æ ¼éªŒè¯APIç«¯ç‚¹"""
        from urllib.parse import urlparse
        
        parsed = urlparse(self.llm.api_base)
        if parsed.path not in ["", "/"]:
            logger.warning(f"âš ï¸ é…ç½®URLåŒ…å«å†—ä½™è·¯å¾„: {parsed.path}")
        
        # ç”Ÿæˆè§„èŒƒåŒ–çš„APIç«¯ç‚¹
        self.llm.api_base = f"{parsed.scheme}://{parsed.netloc}/v1/chat/completions"
        logger.info(f"âœ… æœ€ç»ˆAPIç«¯ç‚¹: {self.llm.api_base}")

        # é¢„æ£€ç«¯ç‚¹æœ‰æ•ˆæ€§
        try:
            response = httpx.get(f"{parsed.scheme}://{parsed.netloc}/v1/models", timeout=10)
            if response.status_code != 200:
                raise ValueError(f"APIç«¯ç‚¹éªŒè¯å¤±è´¥: {response.status_code}")
        except Exception as e:
            logger.critical(f"ğŸ”´ ç«¯ç‚¹éªŒè¯å¤±è´¥: {str(e)}")
            exit(1)

    def generate_text(self, prompt: str, **kwargs) -> str:
        """åŒæ­¥ç”Ÿæˆ"""
        params = self._build_params(prompt, kwargs)
        return self._execute_request(params)

    async def agenerate_text(self, prompt: str, **kwargs) -> str:
        """å¼‚æ­¥ç”Ÿæˆ"""
        params = self._build_params(prompt, kwargs)
        async with self.concurrency_ctl:
            try:
                result = await self._execute_async_request(params)
                await self.concurrency_ctl.adjust(True)
                return result
            except Exception as e:
                await self.concurrency_ctl.adjust(False)
                raise

    def _build_params(self, prompt: str, kwargs: dict) -> dict:
        """æ„å»ºå®‰å…¨å‚æ•°"""
        return {
            "model": "deepseek-ai/deepseek-r1",
            "messages": [{
                "role": "user",
                "content": self._clean_prompt(prompt)
            }],
            "temperature": 0.2,
            "max_tokens": 1024,
            "top_p": 0.9,
            "presence_penalty": 0.5,
            "stream": False
        }

    def _clean_prompt(self, prompt: str) -> str:
        """æ¸…ç†å¤æ‚promptç»“æ„"""
        if isinstance(prompt, tuple):
            return prompt[1].split("Output:")[0].strip()
        return str(prompt).split("Output:")[0].strip()

    @retry(stop=stop_after_attempt(3),
           wait=wait_random_exponential(min=1, max=30),
           retry=retry_if_exception_type((httpx.HTTPError, json.JSONDecodeError)))
    def _execute_request(self, params: dict) -> str:
        """æ‰§è¡ŒåŒæ­¥è¯·æ±‚"""
        try:
            response = httpx.post(
                self.llm.api_base,
                headers=self.headers,
                json=params,
                timeout=60
            )
            return self._process_response(response, params)
        except httpx.HTTPError as e:
            self._log_error(e, params)
            raise

    @retry(stop=stop_after_attempt(3),
           wait=wait_random_exponential(min=1, max=30),
           retry=retry_if_exception_type((httpx.HTTPError, json.JSONDecodeError)))
    async def _execute_async_request(self, params: dict) -> str:
        """æ‰§è¡Œå¼‚æ­¥è¯·æ±‚"""
        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    self.llm.api_base,
                    headers=self.headers,
                    json=params,
                    timeout=60
                )
                return self._process_response(response, params)
            except httpx.HTTPError as e:
                self._log_error(e, params)
                raise

    def _process_response(self, response: httpx.Response, params: dict) -> str:
        """å“åº”å¤„ç†"""
        try:
            response.raise_for_status()
            data = response.json()
            return json.dumps(data["choices"][0]["message"]["content"])
        except Exception as e:
            logger.error(f"ğŸš¨ å“åº”å¤„ç†å¤±è´¥ | URL: {response.url}")
            logger.error(f"âš™ï¸ è¯·æ±‚å‚æ•°: {json.dumps(params, indent=2)}")
            logger.error(f"ğŸ“„ å“åº”å†…å®¹: {response.text[:300]}...")
            raise

    def _log_error(self, e: httpx.HTTPError, params: dict):
        """é”™è¯¯æ—¥å¿—è®°å½•"""
        logger.error(f"""
        === è¯·æ±‚å¤±è´¥è¯¦æƒ… ===
        URL: {e.request.url}
        çŠ¶æ€ç : {e.response.status_code}
        è¯·æ±‚å¤´: {dict(e.request.headers)}
        å‚æ•°: {json.dumps(params, indent=2)}
        å“åº”å†…å®¹: {e.response.text[:300]}...
        """)

# ================== åˆå§‹åŒ–é…ç½® ==================
os.environ["OPENAI_API_KEY"] = "invalid"  # ç¡®ä¿ç¦ç”¨OpenAI

# åˆå§‹åŒ–æ¨¡å‹
config = MODEL_CONFIG["deepseek"]
deepseek_llm = OpenAILike(
    api_base=config['url'],
    api_key=config['api_key'],
    model="deepseek-r1",
    is_chat_model=True,
    temperature=0.2,
    max_tokens=1024,
    timeout=60
)

# ================== è¯„ä¼°é…ç½® ==================
ragas_llm = NvidiaDeepSeekRagasLLM(deepseek_llm)
metrics = [
    ContextRelevance(llm=ragas_llm),
    Faithfulness(llm=ragas_llm),
    AnswerRelevancy(llm=ragas_llm)
]

# ================== æ•°æ®é›†å‡½æ•° ==================
def prepare_eval_dataset():
    """å‡†å¤‡è¯„ä¼°æ•°æ®é›†ï¼ˆé¦–æ¬¡è¿è¡Œéœ€å–æ¶ˆæ³¨é‡Šï¼‰"""
    knowledge_base_id = "ä¿¡è´·ä¸šåŠ¡"
    embedding_model_id = "huggingface_bge-large-zh-v1.5"
    eval_questions = ["è´·åç®¡ç†åŒ…å«å“ªäº›ä¸»è¦å·¥ä½œå†…å®¹ï¼Ÿ"]

    answers, contexts = [], []
    for q in eval_questions:
        try:
            query_engine = utils.load_vector_index(
                knowledge_base_id, 
                embedding_model_id
            ).as_query_engine(llm=deepseek_llm)
            response = query_engine.query(q)
            answers.append(response.response.strip())
            contexts.append([node.text for node in response.source_nodes])
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}")
            answers.append("")
            contexts.append([])

    eval_dataset = Dataset.from_dict({
        "question": eval_questions,
        "answer": answers,
        "contexts": contexts
    })
    eval_dataset.save_to_disk("eval_dataset")
    logger.info("ğŸ’¾ è¯„ä¼°æ•°æ®é›†å·²ä¿å­˜")

# ================== è¯„ä¼°æ‰§è¡Œ ==================
@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(min=1, max=60))
def run_evaluate():
    """æ‰§è¡Œè¯„ä¼°æµç¨‹"""
    # APIè¿é€šæ€§æµ‹è¯•
    try:
        test_prompt = "ç”Ÿæˆæµ‹è¯•å“åº”"
        test_res = ragas_llm.generate_text(test_prompt)
        logger.info(f"ğŸŸ¢ APIæµ‹è¯•å“åº”: {test_res[:50]}...")
    except Exception as e:
        logger.critical(f"ğŸ”´ APIæµ‹è¯•å¤±è´¥: {str(e)}")
        exit(1)

    # åŠ è½½æ•°æ®é›†
    try:
        eval_dataset = load_from_disk("eval_dataset")
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®é›†æˆåŠŸ | æ ·æœ¬æ•°: {len(eval_dataset)}")
    except Exception as e:
        logger.error(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
        exit(1)

    # æ‰§è¡Œè¯„ä¼°
    try:
        result = evaluate(
            eval_dataset,
            metrics=metrics,
            llm=ragas_llm,
            raise_exceptions=False,
            timeout=300
        )
    except Exception as e:
        logger.critical(f"è¯„ä¼°æµç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}")
        exit(1)

    # ç»“æœå®‰å…¨å¤„ç†
    logger.info("\n" + " è¯„ä¼°æŠ¥å‘Š ".center(50, "="))
    
    score_map = {
        'context_relevance': 0.0,
        'faithfulness': 0.0,
        'answer_relevancy': 0.0
    }
    
    for key in score_map.keys():
        if key in result:
            score_map[key] = result[key].mean(skipna=True)
    
    logger.info(f"ä¸Šä¸‹æ–‡ç›¸å…³æ€§: {score_map['context_relevance']:.2%}")
    logger.info(f"å›ç­”å¿ å®åº¦: {score_map['faithfulness']:.2%}")
    logger.info(f"ç­”æ¡ˆç›¸å…³åº¦: {score_map['answer_relevancy']:.2%}")

    logger.info("\nè¯¦ç»†ç»“æœ:")
    print(result.to_pandas().to_markdown(index=False))

if __name__ == "__main__":
    # prepare_eval_dataset()  # é¦–æ¬¡è¿è¡Œå–æ¶ˆæ³¨é‡Š
    run_evaluate()