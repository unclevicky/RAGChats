import sys
import json
import os
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
    retry_if_exception_type
)
import httpx
import numpy as np
from datasets import Dataset, load_from_disk
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    answer_relevancy,
    faithfulness,
    answer_correctness
)
from ragas.llms.base import BaseRagasLLM
from llama_index.llms.openai_like import OpenAILike
from langchain.schema import LLMResult, Generation

project_root = Path(__file__).parent

# é…ç½®æ—¥å¿—è®°å½•
def configure_logging():
    logger = logging.getLogger("NvidiaDeepSeek")
    logger.setLevel(logging.DEBUG)

    if logger.hasHandlers():
        logger.handlers.clear()

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    file_handler = logging.FileHandler("logs/rag_eval.log", encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)

    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s"
    )
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    logger.propagate = False
    
    return logger

logger = configure_logging()
logger.info("âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

# ================== é…ç½®åŠ è½½ ==================
CONFIG_PATH = Path(__file__).parent / "config.json"
with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
    MODEL_CONFIG = json.load(f)

# ================== æ™ºèƒ½å¹¶å‘æ§åˆ¶å™¨ ==================
class AdaptiveConcurrencyController:
    def __init__(self):
        self.semaphore = asyncio.Semaphore(3)
        self.current_delay = 0.0
        self.max_permits = 5
        self.min_permits = 1

    async def adjust(self, success: bool):
        if success:
            self.current_delay = max(0.0, self.current_delay - 1.0)
            if self.semaphore._value < self.max_permits:
                new_permits = min(self.max_permits, self.semaphore._value + 1)
                self.semaphore = asyncio.Semaphore(new_permits)
                logger.info(f"âœ… å¹¶å‘æ•°æå‡è‡³ {new_permits}")
        else:
            self.current_delay += 2.0
            new_permits = max(self.min_permits, self.semaphore._value - 1)
            self.semaphore = asyncio.Semaphore(new_permits)
            logger.warning(f"âš ï¸ å¹¶å‘æ•°é™è‡³ {new_permits} | å»¶è¿Ÿ {self.current_delay:.1f}s")

    async def __aenter__(self):
        await asyncio.sleep(self.current_delay)
        await self.semaphore.acquire()
        return self

    async def __aexit__(self, *args):
        self.semaphore.release()

# ================== NVIDIA DeepSeeké€‚é…å™¨ ==================
class NvidiaDeepSeekRagasLLM(BaseRagasLLM):
    def __init__(self, deepseek_llm: OpenAILike):
        super().__init__()
        self.llm = deepseek_llm
        self.concurrency_ctl = AdaptiveConcurrencyController()
        self._validate_endpoint()
        
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.llm.api_key}",
            "x-api-source": "python-sdk",
            "accept": "application/json"
        }
        logger.info(f"ğŸ”§ åˆå§‹åŒ–æˆåŠŸ | æ¨¡å‹: {self.llm.model}")

    def _validate_endpoint(self):
        from urllib.parse import urlparse
        parsed = urlparse(self.llm.api_base)
        self.llm.api_base = f"{parsed.scheme}://{parsed.netloc}/v1/chat/completions"
        logger.info(f"âœ… APIç«¯ç‚¹: {self.llm.api_base}")

     # æ–°å¢åŒæ­¥ç”Ÿæˆæ–¹æ³•
    def generate_text(
        self,
        prompts: List[str],  # å¿…é¡»ä¿æŒä¸ºç¬¬ä¸€ä¸ªä½ç½®å‚æ•°
        n: int = 1,
        temperature: float = 0.3,
        max_tokens: int = 1024,
        stop: Optional[List[str]] = None,
        **kwargs
        ) -> List[str]:
            """åŒæ­¥ç”Ÿæˆï¼ˆå‚æ•°é¡ºåºä¸¥æ ¼åŒ¹é…ï¼‰"""
            responses = []
            try:
                for prompt in prompts:
                    result = self._execute_sync(
                        prompt=prompt,
                        n=n,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stop=stop
                    )
                    responses.extend(result[:n])
            except Exception as e:
                logger.error(f"åŒæ­¥ç”Ÿæˆå¤±è´¥: {str(e)}")
                raise
            return responses

    async def agenerate_text(
        self,
        prompts: List[str],  # å¿…é¡»ä¿æŒä¸ºç¬¬ä¸€ä¸ªä½ç½®å‚æ•°
        n: int = 1,
        temperature: float = 0.3,
        max_tokens: int = 1024,
        stop: Optional[List[str]] = None,
        **kwargs  # æ·»åŠ kwargså¸æ”¶é¢å¤–å‚æ•°
    ) -> List[str]:
        """å¼‚æ­¥ç”Ÿæˆï¼ˆå‚æ•°é¡ºåºä¸¥æ ¼åŒ¹é…ï¼‰"""
        responses = []
        async with self.concurrency_ctl:
            try:
                tasks = [
                    self._execute_async(
                        prompt=p,
                        n=n,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stop=stop
                    ) for p in prompts
                ]
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for res in results:
                    if isinstance(res, Exception):
                        responses.append("")
                        logger.error(f"å¼‚æ­¥ç”Ÿæˆå¤±è´¥: {str(res)}")
                    else:
                        responses.extend(res[:n])
                
                await self.concurrency_ctl.adjust(True)
            except Exception as e:
                await self.concurrency_ctl.adjust(False)
                logger.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {str(e)}")
                raise
        return responses[:len(prompts)]  # ä¿æŒè¾“å‡ºé•¿åº¦ä¸è¾“å…¥ä¸€è‡´

    def _build_params(
        self,
        prompt: str,
        n: int = 1,
        temperature: float = 0.3,
        max_tokens: int = 1024,
        stop: Optional[List[str]] = None
    ) -> dict:
        params = {
            "model": self.llm.model,
            "messages": [{
                "role": "user",
                "content": self._clean_prompt(prompt)
            }],
            "temperature": max(0.1, min(temperature, 1.0)),
            "max_tokens": min(max_tokens, 4096),
            "n": max(1, n),
            "stop_sequences": stop or []
        }
        params.update({
            "top_p": 0.9,
            "presence_penalty": 0.5
        })
        return params

    def _clean_prompt(self, prompt: str) -> str:
        if isinstance(prompt, (tuple, list)):
            return str(prompt[-1]).split("Output:")[0].strip()
        return str(prompt).split("Output:")[0].strip()

    @retry(stop=stop_after_attempt(3),
           wait=wait_random_exponential(min=1, max=30),
           retry=retry_if_exception_type((httpx.HTTPError, json.JSONDecodeError)))
    def _execute_sync(
        self,
        prompt: str,
        n: int = 1,
        temperature: float = 0.3,
        max_tokens: int = 1024,
        stop: Optional[List[str]] = None
    ) -> List[str]:
        try:
            params = self._build_params(prompt, n, temperature, max_tokens, stop)
            with httpx.Client(timeout=self.llm.timeout) as client:
                response = client.post(
                    self.llm.api_base,
                    headers=self.headers,
                    json=params
                )
                return self._process_response(response, params)
        except httpx.HTTPError as e:
            self._log_error(e, params)
            raise

    @retry(stop=stop_after_attempt(3),
       wait=wait_random_exponential(min=1, max=30),
       retry=retry_if_exception_type((httpx.HTTPError, json.JSONDecodeError, asyncio.TimeoutError)))
    async def _execute_async(
        self,
        prompt: str,
        n: int = 1,
        temperature: float = 0.3,
        max_tokens: int = 1024,
        stop: Optional[List[str]] = None
    ) -> List[str]:
        try:
            params = self._build_params(prompt, n, temperature, max_tokens, stop)
            async with httpx.AsyncClient(
                timeout=httpx.Timeout(300.0, connect=10.0),
                limits=httpx.Limits(max_connections=100)
            ) as client:
                response = await client.post(
                    self.llm.api_base,
                    headers=self.headers,
                    json=params
                )
                return self._process_response(response, params)
        except (httpx.HTTPError, asyncio.TimeoutError) as e:
            self._log_error(e, params)
            raise

    def _process_response(self, response: httpx.Response, params: dict) -> List[str]:
        try:
            response.raise_for_status()
            data = response.json()
            return [
                choice["message"]["content"].strip()
                for choice in data.get("choices", [])[:params.get("n", 1)]
            ]
        except Exception as e:
            logger.error(f"å“åº”è§£æå¤±è´¥: {str(e)}")
            return []

    def _log_error(self, e: httpx.HTTPError, params: dict):
        error_info = {
            "url": str(e.request.url),
            "method": e.request.method,
            "status_code": e.response.status_code if e.response else None,
            "params": params
        }
        logger.error(f"APIè¯·æ±‚å¤±è´¥: {json.dumps(error_info, indent=2)}")

# ================== åˆå§‹åŒ–é…ç½® ==================
os.environ["OPENAI_API_KEY"] = "invalid"

config = MODEL_CONFIG["deepseek"]
deepseek_llm = OpenAILike(
    api_base=config['url'],
    api_key=config['api_key'],
    model=config['model'],
    is_chat_model=True,
    temperature=0.3,
    max_tokens=1024,
    timeout=300,
    http_client=httpx.AsyncClient(
        timeout=httpx.Timeout(300.0, connect=10.0),
        limits=httpx.Limits(max_connections=100)
    )
)

# ================== è¯„ä¼°é…ç½® ==================
ragas_llm = NvidiaDeepSeekRagasLLM(deepseek_llm)
metrics = [
    answer_relevancy,
    faithfulness,
    #answer_correctness
]

# ================== æ•°æ®é›†å‡½æ•° ==================
def prepare_eval_dataset():
    try:
        sys.path.insert(0, str(project_root.parent))
        from backend import utils
        
        knowledge_base_id = "ä¿¡è´·ä¸šåŠ¡"
        embedding_model_id = "huggingface_bge-large-zh-v1.5"
        eval_questions = [
            "ä¿¡è´·å®¡æ‰¹çš„ç‰¹æ®Šæƒ…å½¢æœ‰å“ªäº›ï¼Ÿ",
            "å¦‚ä½•è®¡ç®—è´·æ¬¾åˆ©æ¯ï¼Ÿ",
            "é€¾æœŸè¿˜æ¬¾ä¼šæœ‰ä»€ä¹ˆåæœï¼Ÿ"
        ]

        dataset_dict = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truths": []
        }

        for q in eval_questions:
            try:
                query_engine = utils.load_vector_index(
                    knowledge_base_id, 
                    embedding_model_id
                ).as_query_engine(llm=deepseek_llm)
                response = query_engine.query(q)
                
                dataset_dict["question"].append(q)
                dataset_dict["answer"].append(response.response.strip())
                dataset_dict["contexts"].append([node.text for node in response.source_nodes])
                dataset_dict["ground_truths"].append([])
                
            except Exception as e:
                logger.error(f"é—®é¢˜å¤„ç†å¤±è´¥ [{q}]: {str(e)}")
                continue

        col_lengths = {k: len(v) for k, v in dataset_dict.items()}
        if len(set(col_lengths.values())) != 1:
            raise RuntimeError(f"åˆ—é•¿åº¦ä¸ä¸€è‡´: {col_lengths}")

        eval_dataset = Dataset.from_dict(dataset_dict)
        eval_dataset.save_to_disk(project_root / "eval_dataset")
        logger.info(f"ğŸ’¾ æ•°æ®é›†å·²ä¿å­˜ | æ ·æœ¬æ•°: {len(eval_dataset)}")

    except ImportError:
        logger.error("backendæ¨¡å—ä¸å¯ç”¨")
        exit(1)

# ================== è¯„ä¼°æ‰§è¡Œ ==================
@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(min=1, max=60))
def run_evaluate():
    """å…¼å®¹æ–°ç‰ˆRagasçš„ç»“æœå¤„ç†"""
    logger.info("å¯åŠ¨è¯„ä¼°æµç¨‹".center(50, "="))
    
    try:
        eval_dataset = load_from_disk(project_root / "eval_dataset")
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®é›†æˆåŠŸ | æ ·æœ¬æ•°: {len(eval_dataset)}")
        
        required_columns = {'question', 'answer', 'contexts', 'ground_truths'}
        if not required_columns.issubset(eval_dataset.column_names):
            missing = required_columns - set(eval_dataset.column_names)
            raise ValueError(f"æ•°æ®é›†ç»“æ„æŸåï¼Œç¼ºå¤±å­—æ®µ: {missing}")

    except Exception as e:
        logger.error(f"æ•°æ®é›†é”™è¯¯: {str(e)}")
        exit(1)

    try:
        result = evaluate(
            eval_dataset,
            metrics=metrics,
            llm=ragas_llm,
            raise_exceptions=False
        )
        
        # æ–°ç‰ˆRagasç»“æœå¤„ç†æ–¹å¼
        logger.info("\n" + " è¯„ä¼°æŠ¥å‘Š ".center(50, "="))
        if hasattr(result, 'to_pandas'):
            df = result.to_pandas()
            for metric in metrics:
                col_name = metric.name
                if col_name in df.columns:
                    scores = df[col_name].dropna()
                    if not scores.empty:
                        logger.info(f"{col_name}:")
                        logger.info(f"  å¹³å‡å€¼: {scores.mean():.2%}")
                        logger.info(f"  ä¸­ä½æ•°: {scores.median():.2%}")
                        logger.info(f"  æ ‡å‡†å·®: {scores.std():.2f}")
                    else:
                        logger.warning(f"{col_name}: æ— æœ‰æ•ˆæ•°æ®")
        else:
            logger.error("æ— æ³•è¯†åˆ«è¯„ä¼°ç»“æœæ ¼å¼")
            
    except Exception as e:
        logger.critical(f"è¯„ä¼°å¤±è´¥: {str(e)}", exc_info=True)
        exit(1)

if __name__ == "__main__":
    # prepare_eval_dataset()  # é¦–æ¬¡è¿è¡Œæ—¶å–æ¶ˆæ³¨é‡Š
    run_evaluate()